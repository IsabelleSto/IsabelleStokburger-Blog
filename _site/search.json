[
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html",
    "title": "Hate Speech Klassifikation",
    "section": "",
    "text": "Die Klassifikation von Hate Speech mit R in der Version 4.3.1 bietet eine effiziente Methode, um digitale Kommunikation auf toxische Inhalte zu analysieren. Das Stastikprogramm R eignet sich gut zur Verarbeitung natürlicher Sprache und maschinellen Lernen, um Hate Speech in Textdaten zu identifizieren und zu kategorisieren. Im Folgenden werden mit Hilfe verschiedener Textmining Methoden Tweets auf Hate Speech hin untersucht. Darüber hinaus werden prädiktive Modelle zur Klassifikation von Hate-Speech angewandt."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#daten-laden",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#daten-laden",
    "title": "Hate Speech Klassifikation",
    "section": "2.1 Daten laden",
    "text": "2.1 Daten laden\n\nd_hate&lt;-\n  read.csv(datenpfad)\n\n\n2.1.1 Pakete laden\n\nlibrary(tidyverse)\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(beepr)\nlibrary(discrim)\nlibrary(naivebayes)\nlibrary(textrecipes) \nlibrary(syuzhet)\nlibrary(tokenizers)  \nlibrary(tidytext)  \nlibrary(SnowballC)  \nlibrary(lsa)  \nlibrary(easystats)  \nlibrary(textclean)  \nlibrary(wordcloud)  \nlibrary(ggplot2)\nlibrary(textrecipes)\nlibrary(workflowsets)\nlibrary(sentimentr)\nlibrary(textdata)\nlibrary(tm)\nlibrary(stringr)\nlibrary(readr)\n\n\nd_hate1&lt;-\n  d_hate%&gt;%\n  select(tweet, class)%&gt;%\n  mutate(id = as.character(1:nrow(.)))\n\nd_hate1%&gt;%\n  count(class)\n\n\n\n  \n\n\n# Anzahl der Tweets pro Klasse zählen\nclass_counts &lt;- d_hate1 %&gt;%\n  count(class)\n\n# Histogramm erstellen\nggplot(class_counts, aes(x = class, y = n, fill = class)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"hatespeech\" = \"yellow\", \"other\" = \"cyan\")) +\n  labs(title = \"Verteilung der Tweets nach Klasse\",\n       x = \"Klasse\",\n       y = \"Anzahl der Tweets\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nDas Balkendiagramm zeigt die Verteilung der Tweet in den zwei Klassen “hate_speech” und “other”. Rund 25% der Tweets sind im Datensatz als Hate Speech gelabelt."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#bestes-modell-extrahieren",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#bestes-modell-extrahieren",
    "title": "Hate Speech Klassifikation",
    "section": "7.4 Bestes Modell extrahieren",
    "text": "7.4 Bestes Modell extrahieren\n\nbest_ml &lt;- \n  extract_workflow_set_result(results, \"text_prep_rf\") %&gt;% \n  select_best()\n\nbest_wf &lt;- \nwf_set %&gt;% \n  extract_workflow(\"text_prep_rf\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_ml)\n\nfit_final &lt;- fit(best_wf_finalized, data = d_train)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IsabelleStokburger Blog",
    "section": "",
    "text": "Hate Speech Klassifikation\n\n\n\n\n\n\n\nTextanalyse\n\n\nTidymodels\n\n\nKlassifikation\n\n\nTransformers\n\n\nNeuronale Netze\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nIsabelle Stokburger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#aufteilung-in-test--und-traindatensatz",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#aufteilung-in-test--und-traindatensatz",
    "title": "Hate Speech Klassifikation",
    "section": "3.1 Aufteilung in Test- und Traindatensatz",
    "text": "3.1 Aufteilung in Test- und Traindatensatz\n\nset.seed(123)\nd_split &lt;- initial_split(d_hate1, prop = .8, strata = class)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n3.1.1 Seiten und Wörter zählen\n\nstr_count(d_train$tweet, pattern = \"\\\\w\") %&gt;% sum(na.rm = TRUE)\n\n[1] 295528\n\n\nDie zu untersuchende Zeile “Tweet” enthält XYXYXYX Wörter. Das ist eine ausreichende Menge, um ein Modell sinnvoll trainieren zu können.\n\nNachstehend werden verschiedene Textmerkmale, auch als Textfeatures bezeichnet, untersucht. Durch das Analysieren der spezifischen Eigenschaften können Informationen über den Inhalt, die Struktur und die Bedeutung der Tweets gewonnen werden. Das hilft dabei, im späteren Verlauf Hassrede zu identifizieren. Anschließend werden die Tweets auf * Wortfrequenzen * Schimpfwörter * Emotionale Ladung untersucht."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#tokenisierung",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#tokenisierung",
    "title": "Hate Speech Klassifikation",
    "section": "3.2 Tokenisierung",
    "text": "3.2 Tokenisierung\n\ntokens &lt;- d_train %&gt;%\n  unnest_tokens(word, tweet)\n\nDie Tokenisierung stellt einen wichtigen Schritt in der Textverarbeitung dar. Sie teilt den Text in sinnvolle Einheiten, sogenannte Tokens auf. Erst das ermöglicht weitere Verarbeitungsschritte, wie beispielsweise die Stoppwordentfernung."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#entfernen-von-stopwords",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#entfernen-von-stopwords",
    "title": "Hate Speech Klassifikation",
    "section": "3.3 Entfernen von Stopwords",
    "text": "3.3 Entfernen von Stopwords\n\n3.3.1 Stopwords laden\n\ndata(stopwords_de, package = \"lsa\")\ndata(stopwords_en, package = \"lsa\")\nstopwords_en &lt;- tibble(word = stopwords_en)\nstopwords_de &lt;- tibble(word = stopwords_de)\nstopwords &lt;- bind_rows(stopwords_de, stopwords_en)\n\n\ntokens_filtered &lt;- tokens %&gt;%\n  anti_join(stopwords, by = \"word\")\n\nDas Entfernen von stopwords hat den Vorteil, dass durch das Entfernen häufig vorkommende Wörter ohne semantische Bedeutung wie “a”, “and”, “so”… Rauschen im Text reduziert und die Analysequalität verbessert wird. Darüber hinaus verringert sich die zu verarbeitende Datenmenge."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#textlänge",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#textlänge",
    "title": "Hate Speech Klassifikation",
    "section": "3.4 Textlänge",
    "text": "3.4 Textlänge\n\ntext_length &lt;- tokens_filtered %&gt;%\n  group_by(id) %&gt;%\n  summarise(word_count = n())\n\nDie Textlänge kann Aufschluss über die Menge und Vielfalt der Informationen der Tweets geben. Gibt es möglicherweisee ungewöhnliche kurze oder lange Tweets, die genauer betrachtet werden sollten?"
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#worthäufigkeit",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#worthäufigkeit",
    "title": "Hate Speech Klassifikation",
    "section": "3.5 Worthäufigkeit",
    "text": "3.5 Worthäufigkeit\n\nword_freq &lt;- tokens_filtered %&gt;%\n  count(word, sort = TRUE)\n\nprint(head(text_length, n = 20)) # Die Textlänge der ersten 10 Tweets\nprint(head(word_freq, n = 20))   # Die 10 häufigsten Wörter\n\n#Berechnung der Wortfrequenz\nword_freq_filtered &lt;- tokens_filtered %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  top_n(20, n)\n\n\n3.5.1 Visualisierung der Textlänge\n\ntext_length_filtered &lt;- tokens_filtered %&gt;%\n  group_by(id) %&gt;%\n  summarise(word_count = n())\n\n# Aktualisierte Visualisierung: Histogramm der Textlänge ohne Stoppwörter\nggplot(text_length_filtered, aes(x = word_count)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"white\") +\n  theme_minimal() +\n  labs(title = \"Histogramm der Textlänge ohne Stoppwörter\", x = \"Anzahl der Wörter pro Tweet\", y = \"Häufigkeit\")\n\n\n\n\nDie Tweets bewegen sich alle in einem ähnlichen Rahmen und sind i.d.R zwischen drei und 13 Wörtern lang. Da es nicht viele Aussreißer gibt, werde ich es später im Rezept nicht beachten."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung-1-barplot-der-häufigsten-wörter-ohne-stopwords",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung-1-barplot-der-häufigsten-wörter-ohne-stopwords",
    "title": "Hate Speech Klassifikation",
    "section": "3.6 Visualisierung 1: Barplot der häufigsten Wörter ohne Stopwords",
    "text": "3.6 Visualisierung 1: Barplot der häufigsten Wörter ohne Stopwords\n\nggplot(word_freq_filtered, aes(x = reorder(word, n), y = n)) +\n  geom_col(fill = \"coral\", color = \"white\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Top 20 häufigste Wörter\", x = \"\", y = \"Häufigkeit\")\n\n\n\n\nAllein unter den 20 häufigsten Wörtern befinden sich bereits sieben Schimpfwörter. Die Genauigkeit der Grafik könnte noch verbessert werden, indem zum Beispiel ähnliche Ausdrücke wie “nigga” und “nigger” zusammengefasst werden, sodass der Barplot die kummulierte Häufigkeit anzeigt. Zusätzlich könnten weitere stopwords wie “it” durch die Hinzunahme eines weiteren Stopwords-Datensatzes entfernt werden.\n\n3.6.1 Wörter ohne Aussagekraft aus tokenisierten Daten entfernen\n\ntokens_filtered &lt;- tokens_filtered %&gt;%\n  filter(word != \"rt\")\n\nDa der Ausdruck “rt” keine Aussagekraft hat, wird er aus dem Datensatz entfernt."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#berechnung-der-durchschnittlichen-sentimentwerte-pro-polarität-und-tweet",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#berechnung-der-durchschnittlichen-sentimentwerte-pro-polarität-und-tweet",
    "title": "Hate Speech Klassifikation",
    "section": "4.1 Berechnung der durchschnittlichen Sentimentwerte pro Polarität und Tweet",
    "text": "4.1 Berechnung der durchschnittlichen Sentimentwerte pro Polarität und Tweet\n\ntokens_senti2 &lt;-\n  tokens_senti %&gt;% \n  group_by(id, neg_pos) %&gt;% \n  summarise(senti_avg = mean(value))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nhead(tokens_senti2)"
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#zusammenführung-der-sentimentwerte-und-textlänge-in-den-hauptdatensatz",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#zusammenführung-der-sentimentwerte-und-textlänge-in-den-hauptdatensatz",
    "title": "Hate Speech Klassifikation",
    "section": "4.2 Zusammenführung der Sentimentwerte und Textlänge in den Hauptdatensatz",
    "text": "4.2 Zusammenführung der Sentimentwerte und Textlänge in den Hauptdatensatz\n\nsentis_wide &lt;-\n  tokens_senti2 %&gt;% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nsentis_wide %&gt;% head()\n\n#Zusammenführung mit Ursprungsdatensatz\nd_train2&lt;-\n  d_train%&gt;%\n  full_join(sentis_wide)\n\nd_train2 &lt;- d_train2 %&gt;%\n  left_join(text_length, by = \"id\")\n\n\nsenti_afinn%&gt;%\n  select(value, neg_pos)%&gt;%\n  describe_distribution()\n\n\n\n  \n\n\ntokens_senti %&gt;% \n  summarise(senti_sum = mean(value) %&gt;% round(2))\n\n\n\n  \n\n\n\nDas Sentimentlexikon ist insgesamt mit einem Wert von -0.59 leicht negativ. Die Tweets liegen mit -1.01 deutlich im negativen Bereich, was auf eine überwiegend negative Stimmung hindeutet. Das kann einmal auf das Thema der Diskussion zurückzuführen sein oder auch auf den gehäuften Gebrauch von Sarkasmus und Ironie."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung-der-sentimentanalyse",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung-der-sentimentanalyse",
    "title": "Hate Speech Klassifikation",
    "section": "4.3 Visualisierung der Sentimentanalyse",
    "text": "4.3 Visualisierung der Sentimentanalyse\n\nlibrary(ggplot2)\n\ntokens_senti %&gt;%\n  count(word, neg_pos, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  group_by(neg_pos) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word, fill = neg_pos)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos, scales = \"free_y\") +\n  labs(x = \"Häufigkeit\",\n       y = \"Wort\") +\n  theme_minimal()\n\n\n\n\nAuffällig ist, dass die negativ behafteten Wörter insgesamt deutlich häufiger vorkommen, als die positiven. Das passt mit dem vorherigen Ergebnis zusammen. Wenigsten wird der Ausdruck “love” häufiger verwendet als “hate."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung-der-bigramme",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung-der-bigramme",
    "title": "Hate Speech Klassifikation",
    "section": "5.2 Visualisierung der Bigramme",
    "text": "5.2 Visualisierung der Bigramme\n\n# Visualisierung der Top-Bigramme\nggplot(top_bigrams, aes(x = reorder(bigram, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 häufigste Bigramme\",\n       x = \"Bigramme\",\n       y = \"Häufigkeit\") +\n  theme_minimal()\n\n\n\n\nAnscheinend spielen Charlies eine große Rolle… besonders aussagekräftig ist das nicht. Hierfür wäre ein Bigram-Netz sinnvoll, um die Beziehungen der Wörter zueinander zu zeigen."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#stemming",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#stemming",
    "title": "Hate Speech Klassifikation",
    "section": "5.3 Stemming",
    "text": "5.3 Stemming\n\n# Stemming\ntokens_filtered$word &lt;- wordStem(tokens_filtered$word, language = \"en\")\n\n# Häufigkeiten zählen\nword_freq_stem &lt;- tokens_filtered %&gt;%\n  count(word, sort = TRUE)\n\n# Die 10 häufigsten Wörter auswählen\ntop_words_stem &lt;- head(word_freq_stem, 10)\n\n# Visualisierung\nggplot(top_words_stem, aes(x = reorder(word, n), y = n, fill = word)) +\n  geom_col() +\n  coord_flip() +  # Um die Wörter horizontal anzuzeigen\n  labs(title = \"Top 10 gestemmte Wörter\", x = \"Wörter\", y = \"Häufigkeit\") +\n  scale_fill_viridis_d() +\n  theme_minimal()\n\n\n\n\nStemming habe ich durchgeführt, weil es die Anzahl der verschiedenen Wortformen reduziert, indem diese auf einen gemeinsamen Wortstamm zurückgeführt werden. Damit kann Rauschen reduziert werden und die Vergleichbarkeit erhöht. Ich hatte mir erhofft, häufig vorkommendene Wortgruppe zu extrahieren, Rauschen zu reduzieren und die Vergleichbarkeit der Wörter zu erhöhen. Leider hat der Output keine große Aussagekraft und nicht den gewünschten Effekt erzielt. Zumindest ist es bunt und hebt die Stimmung für weiter Analysen."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#rezept",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#rezept",
    "title": "Hate Speech Klassifikation",
    "section": "7.2 Rezept",
    "text": "7.2 Rezept\nDas Rezept enthält die Sentimentwerte, Schimpfwörter und tfidf. TFIDF wird zusätzlich mit aufgenommen, weil es nützlich zur Bewertung der Wichtigkeit von Begriffen ist.\n\nlibrary(readxl)\nInsults_English &lt;-\n  read_excel(datenpfad2)\n\n\nInsults &lt;-\n  Insults_English %&gt;%\n  rename(word = abomination)\n\nInsults$value &lt;- 1\n\nRezept definieren\n\nlibrary(syuzhet)\n\nrec1 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;%\n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(schimpf_w = get_sentiment(tweet,\n                                        method = \"custom\",\n                                        lexicon = Insults)) %&gt;%\n  step_mutate(senti = get_sentiment(tweet, \n                                    method = \"nrc\",\n                                    language = \"english\")) %&gt;% \n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem(tweet) %&gt;%  \n  step_tfidf(tweet)\n\nIch habe mich nun für einn anderes Schimpfwort-Wörterbuch entschieden. Das neue von stammt von https://www.insult.wiki/list-of-insults und hat zehnmal mehr Wörter im Vergleich zum “schimpf” Wörterbuch.\n\nbaked &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked\n\n\n\n  \n\n\n\nModell\n\nlm &lt;- naive_Bayes() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"naivebayes\")\nlm\n\nKreuzvalidierung\n\nset.seed(42)\nfolds1 &lt;- vfold_cv(d_train, v = 5)\n\nWorkflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec1) %&gt;% \n  add_model(lm)\n\nFitting\n\nfit1 &lt;-\n  fit_resamples(\n    wf1,\n    folds1,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nPerformance evaluieren\n\nwf1_performance &lt;-\n  collect_metrics(fit1)\nwf1_performance\n\n\n\n  \n\n\nwf_preds &lt;-\n  collect_predictions(fit1)\nwf_preds\n\n\n\n  \n\n\n\nVisualisierung\n\nwf_preds %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(truth = class, .pred_other) %&gt;% \n  autoplot()\n\n\n\nconf_mat_resampled(fit1, tidy = FALSE) %&gt;% \n  autoplot(type = \"heatmap\")\n\n\n\n\nPuh, das ist ziemlich unterirdisch. Das Modell hat eine sehr geringe Sensitivität. Das bedeutet, dass es schlecht darin ist Hate Speech zu identifizieren und schlechter als ein zufälliger Klassifikator ist. Es sagt falsche Klassen vorher Ziel ist im weiteren Verlauf ein Modell aufzustellen, dessen ROC-Kurce nahe der y-Achse und an der oberen Grenze des Diagramms liegt. Dann hätte es eine hohe Sensitivität und Spezifität. Die Heatmap bestätigt das Ergebnis der ROC-Kurce."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage-mit-workflow-set",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage-mit-workflow-set",
    "title": "Hate Speech Klassifikation",
    "section": "7.3 2. Vorhersage mit Workflow-Set",
    "text": "7.3 2. Vorhersage mit Workflow-Set\n\nrec2&lt;-\n  recipe(class ~ ., data = d_train) %&gt;%\n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem(tweet) %&gt;%  \n  step_tfidf(tweet)\n\nModelle definieren\n\nlibrary(foreach)\n\nWarning: Paket 'foreach' wurde unter R Version 4.3.2 erstellt\n\n\n\nAttache Paket: 'foreach'\n\n\nDie folgenden Objekte sind maskiert von 'package:purrr':\n\n    accumulate, when\n\nlibrary(purrr)\n\nif (!requireNamespace(\"doParallel\", quietly = TRUE)) {\n  install.packages(\"doParallel\")\n}\n\nlibrary(doParallel)\n\nLade nötiges Paket: iterators\n\n\nWarning: Paket 'iterators' wurde unter R Version 4.3.2 erstellt\n\n\nLade nötiges Paket: parallel\n\nregisterDoParallel(cores = detectCores())\n\nModelle definieren\n\nmodel_xgb &lt;- boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nmodel_kknn &lt;- nearest_neighbor(neighbors = tune()) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\nmodel_rf &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Workflow-Set erstellen\nwf_set &lt;- workflow_set(\n  preproc = list(text_prep = rec2),\n  models = list(xgb = model_xgb, kknn = model_kknn, rf = model_rf),\n  cross = TRUE\n)\n\nUm Rechenzeit zu sparen, tune ich nur den XGBoost. Ich habe den XGBoost zum Tunen ausgesucht, weil dieser Algorithmen die meisten Tuningparamter besitzt und seine Leistung im Vergleich oft am besten abschneidet.\nResampling und Tuning\n\n# Resampling definieren\nset.seed(123)\ncv_folds &lt;- vfold_cv(d_hate, v = 3, strata = class)\n\n# Tuning und Auswahl des besten Modells\nresults &lt;- wf_set %&gt;%\n  workflow_map(\n    fn = \"tune_grid\",\n    resamples = cv_folds,\n    grid = 5,\n    seed = 42,\n    metrics = metric_set(roc_auc),\n    verbose = TRUE, \n    control = control_resamples(save_pred = TRUE)\n  )\n\ni 1 of 3 tuning:     text_prep_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 1 of 3 tuning:     text_prep_xgb (36.9s)\n\n\ni 2 of 3 tuning:     text_prep_kknn\n\n\n✔ 2 of 3 tuning:     text_prep_kknn (10.9s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 3 of 3 resampling: text_prep_rf\n\n\n✔ 3 of 3 resampling: text_prep_rf (4.8s)\n\ntune::autoplot(results) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nresults %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n\n\n  \n\n\n\nObwohl ich den RandomForest nicht getuned habe, schneidet dieser am besten ab mit einem ROC von 0.87 und somit im Vergleich zum LM auch deutlich besser."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#visualisierung",
    "title": "Hate Speech Klassifikation",
    "section": "7.5 Visualisierung",
    "text": "7.5 Visualisierung\n\nlibrary(ggplot2)\n\n# Vorhersagen für den Testdatensatz machen, wenn noch nicht geschehen\npredictions &lt;- predict(fit_final, new_data = d_test, type = \"prob\")\n\n# Vorhersage-Verteilung visualisieren\npredictions %&gt;%\n  bind_cols(d_test %&gt;% select(class)) %&gt;%\n  ggplot(aes(x = `.pred_hate speech`, fill = class)) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30) +\n  labs(x = \"Predicted Probability\", y = \"Count\", fill = \"Actual Class\")\n\n\n\n\nIm Idealfall wäre links nur blaue Balken und rechts nur rote, sodass es keine Überschneidung gibt. Dann wäre die Vorhersage sehr genau und Hate Speech würde korrekt erkannt werden. Obwohl hier kein Sentiment- & Schimpfwortanalyse durcchgeführt wurde, schneidet der RandomForest Algorithmus besser ab, als die anderen drei."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage",
    "title": "Hate Speech Klassifikation",
    "section": "9.1 3. Vorhersage",
    "text": "9.1 3. Vorhersage"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ dataset_summary | default(““, true) }}\n\n\n\n\n\n{{ dataset_description | default(““, true) }}\n\nCurated by: {{ curators | default(“[More Information Needed]”, true)}}\nFunded by [optional]: {{ funded_by | default(“[More Information Needed]”, true)}}\nShared by [optional]: {{ shared_by | default(“[More Information Needed]”, true)}}\nLanguage(s) (NLP): {{ language | default(“[More Information Needed]”, true)}}\nLicense: {{ license | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nRepository: {{ repo | default(“[More Information Needed]”, true)}}\nPaper [optional]: {{ paper | default(“[More Information Needed]”, true)}}\nDemo [optional]: {{ demo | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n\n\n{{ direct_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ out_of_scope_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n{{ dataset_structure | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ curation_rationale_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n{{ data_collection_and_processing_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ source_data_producers_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n{{ annotation_process_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ who_are_annotators_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ personal_and_sensitive_information | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ bias_risks_limitations | default(“[More Information Needed]”, true)}}\n\n\n\n{{ bias_recommendations | default(“Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.”, true)}}\n\n\n\n\n\nBibTeX:\n{{ citation_bibtex | default(“[More Information Needed]”, true)}}\nAPA:\n{{ citation_apa | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ glossary | default(“[More Information Needed]”, true)}}\n\n\n\n{{ more_information | default(“[More Information Needed]”, true)}}\n\n\n\n{{ dataset_card_authors | default(“[More Information Needed]”, true)}}\n\n\n\n{{ dataset_card_contact | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-details",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-details",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ dataset_description | default(““, true) }}\n\nCurated by: {{ curators | default(“[More Information Needed]”, true)}}\nFunded by [optional]: {{ funded_by | default(“[More Information Needed]”, true)}}\nShared by [optional]: {{ shared_by | default(“[More Information Needed]”, true)}}\nLanguage(s) (NLP): {{ language | default(“[More Information Needed]”, true)}}\nLicense: {{ license | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nRepository: {{ repo | default(“[More Information Needed]”, true)}}\nPaper [optional]: {{ paper | default(“[More Information Needed]”, true)}}\nDemo [optional]: {{ demo | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#uses",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#uses",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ direct_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ out_of_scope_use | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-structure",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-structure",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ dataset_structure | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-creation",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-creation",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ curation_rationale_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n{{ data_collection_and_processing_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ source_data_producers_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n{{ annotation_process_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ who_are_annotators_section | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ personal_and_sensitive_information | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#bias-risks-and-limitations",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#bias-risks-and-limitations",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ bias_risks_limitations | default(“[More Information Needed]”, true)}}\n\n\n\n{{ bias_recommendations | default(“Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#citation-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#citation-optional",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "BibTeX:\n{{ citation_bibtex | default(“[More Information Needed]”, true)}}\nAPA:\n{{ citation_apa | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#glossary-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#glossary-optional",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ glossary | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#more-information-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#more-information-optional",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ more_information | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-card-authors-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-card-authors-optional",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ dataset_card_authors | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-card-contact",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/datasetcard_template.html#dataset-card-contact",
    "title": "1 Dataset Card for {{ pretty_name | default(“Dataset Name”, true) }}",
    "section": "",
    "text": "{{ dataset_card_contact | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ model_summary | default(““, true) }}\n\n\n\n\n\n{{ model_description | default(““, true) }}\n\nDeveloped by: {{ developers | default(“[More Information Needed]”, true)}}\nFunded by [optional]: {{ funded_by | default(“[More Information Needed]”, true)}}\nShared by [optional]: {{ shared_by | default(“[More Information Needed]”, true)}}\nModel type: {{ model_type | default(“[More Information Needed]”, true)}}\nLanguage(s) (NLP): {{ language | default(“[More Information Needed]”, true)}}\nLicense: {{ license | default(“[More Information Needed]”, true)}}\nFinetuned from model [optional]: {{ base_model | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nRepository: {{ repo | default(“[More Information Needed]”, true)}}\nPaper [optional]: {{ paper | default(“[More Information Needed]”, true)}}\nDemo [optional]: {{ demo | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n\n{{ direct_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ downstream_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ out_of_scope_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n{{ bias_risks_limitations | default(“[More Information Needed]”, true)}}\n\n\n\n{{ bias_recommendations | default(“Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.”, true)}}\n\n\n\n\nUse the code below to get started with the model.\n{{ get_started_code | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ training_data | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ preprocessing | default(“[More Information Needed]”, true)}}\n\n\n\n\nTraining regime: {{ training_regime | default(“[More Information Needed]”, true)}} \n\n\n\n\n\n{{ speeds_sizes_times | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n\n\n\n\n\n{{ testing_data | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ testing_factors | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ testing_metrics | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ results | default(“[More Information Needed]”, true)}}\n\n\n{{ results_summary | default(““, true) }}\n\n\n\n\n\n\n{{ model_examination | default(“[More Information Needed]”, true)}}\n\n\n\n\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\n\nHardware Type: {{ hardware_type | default(“[More Information Needed]”, true)}}\nHours used: {{ hours_used | default(“[More Information Needed]”, true)}}\nCloud Provider: {{ cloud_provider | default(“[More Information Needed]”, true)}}\nCompute Region: {{ cloud_region | default(“[More Information Needed]”, true)}}\nCarbon Emitted: {{ co2_emitted | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ model_specs | default(“[More Information Needed]”, true)}}\n\n\n\n{{ compute_infrastructure | default(“[More Information Needed]”, true)}}\n\n\n{{ hardware_requirements | default(“[More Information Needed]”, true)}}\n\n\n\n{{ software | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nBibTeX:\n{{ citation_bibtex | default(“[More Information Needed]”, true)}}\nAPA:\n{{ citation_apa | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ glossary | default(“[More Information Needed]”, true)}}\n\n\n\n{{ more_information | default(“[More Information Needed]”, true)}}\n\n\n\n{{ model_card_authors | default(“[More Information Needed]”, true)}}\n\n\n\n{{ model_card_contact | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-details",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-details",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ model_description | default(““, true) }}\n\nDeveloped by: {{ developers | default(“[More Information Needed]”, true)}}\nFunded by [optional]: {{ funded_by | default(“[More Information Needed]”, true)}}\nShared by [optional]: {{ shared_by | default(“[More Information Needed]”, true)}}\nModel type: {{ model_type | default(“[More Information Needed]”, true)}}\nLanguage(s) (NLP): {{ language | default(“[More Information Needed]”, true)}}\nLicense: {{ license | default(“[More Information Needed]”, true)}}\nFinetuned from model [optional]: {{ base_model | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\nRepository: {{ repo | default(“[More Information Needed]”, true)}}\nPaper [optional]: {{ paper | default(“[More Information Needed]”, true)}}\nDemo [optional]: {{ demo | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#uses",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#uses",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ direct_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ downstream_use | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ out_of_scope_use | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#bias-risks-and-limitations",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#bias-risks-and-limitations",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ bias_risks_limitations | default(“[More Information Needed]”, true)}}\n\n\n\n{{ bias_recommendations | default(“Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#how-to-get-started-with-the-model",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#how-to-get-started-with-the-model",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "Use the code below to get started with the model.\n{{ get_started_code | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#training-details",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#training-details",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ training_data | default(“[More Information Needed]”, true)}}\n\n\n\n\n\n\n{{ preprocessing | default(“[More Information Needed]”, true)}}\n\n\n\n\nTraining regime: {{ training_regime | default(“[More Information Needed]”, true)}} \n\n\n\n\n\n{{ speeds_sizes_times | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#evaluation",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#evaluation",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ testing_data | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ testing_factors | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ testing_metrics | default(“[More Information Needed]”, true)}}\n\n\n\n\n{{ results | default(“[More Information Needed]”, true)}}\n\n\n{{ results_summary | default(““, true) }}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-examination-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-examination-optional",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ model_examination | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#environmental-impact",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#environmental-impact",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\n\nHardware Type: {{ hardware_type | default(“[More Information Needed]”, true)}}\nHours used: {{ hours_used | default(“[More Information Needed]”, true)}}\nCloud Provider: {{ cloud_provider | default(“[More Information Needed]”, true)}}\nCompute Region: {{ cloud_region | default(“[More Information Needed]”, true)}}\nCarbon Emitted: {{ co2_emitted | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#technical-specifications-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#technical-specifications-optional",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ model_specs | default(“[More Information Needed]”, true)}}\n\n\n\n{{ compute_infrastructure | default(“[More Information Needed]”, true)}}\n\n\n{{ hardware_requirements | default(“[More Information Needed]”, true)}}\n\n\n\n{{ software | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#citation-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#citation-optional",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "BibTeX:\n{{ citation_bibtex | default(“[More Information Needed]”, true)}}\nAPA:\n{{ citation_apa | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#glossary-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#glossary-optional",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ glossary | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#more-information-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#more-information-optional",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ more_information | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-card-authors-optional",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-card-authors-optional",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ model_card_authors | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-card-contact",
    "href": "viren/Lib/site-packages/huggingface_hub/templates/modelcard_template.html#model-card-contact",
    "title": "1 Model Card for {{ model_id | default(“Model ID”, true) }}",
    "section": "",
    "text": "{{ model_card_contact | default(“[More Information Needed]”, true)}}"
  },
  {
    "objectID": "viren/Lib/site-packages/idna-3.6.dist-info/LICENSE.html",
    "href": "viren/Lib/site-packages/idna-3.6.dist-info/LICENSE.html",
    "title": "IsabelleStokburger Blog",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2023, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "viren/Lib/site-packages/Markdown-3.5.2.dist-info/LICENSE.html",
    "href": "viren/Lib/site-packages/Markdown-3.5.2.dist-info/LICENSE.html",
    "title": "IsabelleStokburger Blog",
    "section": "",
    "text": "Copyright 2007, 2008 The Python Markdown Project (v. 1.7 and later) Copyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b) Copyright 2004 Manfred Stienstra (the original version)\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the Python Markdown Project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE PYTHON MARKDOWN PROJECT ‘’AS IS’’ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL ANY CONTRIBUTORS TO THE PYTHON MARKDOWN PROJECT BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "viren/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "viren/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "1 NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\n1 NCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n2 3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n3 Components\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "viren/Lib/site-packages/tensorflow/include/external/libjpeg_turbo/LICENSE.html",
    "href": "viren/Lib/site-packages/tensorflow/include/external/libjpeg_turbo/LICENSE.html",
    "title": "1 libjpeg-turbo Licenses",
    "section": "",
    "text": "1 libjpeg-turbo Licenses\nlibjpeg-turbo is covered by three compatible BSD-style open source licenses:\n\nThe IJG (Independent JPEG Group) License, which is listed in README.ijg\nThis license applies to the libjpeg API library and associated programs (any code inherited from libjpeg, and any modifications to that code.)\nThe Modified (3-clause) BSD License, which is listed below\nThis license covers the TurboJPEG API library and associated programs, as well as the build system.\nThe zlib License\nThis license is a subset of the other two, and it covers the libjpeg-turbo SIMD extensions.\n\n\n\n2 Complying with the libjpeg-turbo Licenses\nThis section provides a roll-up of the libjpeg-turbo licensing terms, to the best of our understanding.\n\nIf you are distributing a modified version of the libjpeg-turbo source, then:\n\nYou cannot alter or remove any existing copyright or license notices from the source.\nOrigin\n\nClause 1 of the IJG License\nClause 1 of the Modified BSD License\nClauses 1 and 3 of the zlib License\n\nYou must add your own copyright notice to the header of each source file you modified, so others can tell that you modified that file (if there is not an existing copyright header in that file, then you can simply add a notice stating that you modified the file.)\nOrigin\n\nClause 1 of the IJG License\nClause 2 of the zlib License\n\nYou must include the IJG README file, and you must not alter any of the copyright or license text in that file.\nOrigin\n\nClause 1 of the IJG License\n\n\nIf you are distributing only libjpeg-turbo binaries without the source, or if you are distributing an application that statically links with libjpeg-turbo, then:\n\nYour product documentation must include a message stating:\nThis software is based in part on the work of the Independent JPEG Group.\nOrigin\n\nClause 2 of the IJG license\n\nIf your binary distribution includes or uses the TurboJPEG API, then your product documentation must include the text of the Modified BSD License (see below.)\nOrigin\n\nClause 2 of the Modified BSD License\n\n\nYou cannot use the name of the IJG or The libjpeg-turbo Project or the contributors thereof in advertising, publicity, etc.\nOrigin\n\nIJG License\nClause 3 of the Modified BSD License\n\nThe IJG and The libjpeg-turbo Project do not warrant libjpeg-turbo to be free of defects, nor do we accept any liability for undesirable consequences resulting from your use of the software.\nOrigin\n\nIJG License\nModified BSD License\nzlib License\n\n\n\n\n3 The Modified (3-clause) BSD License\nCopyright (C)2009-2022 D. R. Commander. All Rights Reserved. Copyright (C)2015 Viktor Szathmáry. All Rights Reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the libjpeg-turbo Project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS”, AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n4 Why Three Licenses?\nThe zlib License could have been used instead of the Modified (3-clause) BSD License, and since the IJG License effectively subsumes the distribution conditions of the zlib License, this would have effectively placed libjpeg-turbo binary distributions under the IJG License. However, the IJG License specifically refers to the Independent JPEG Group and does not extend attribution and endorsement protections to other entities. Thus, it was desirable to choose a license that granted us the same protections for new code that were granted to the IJG for code derived from their software."
  },
  {
    "objectID": "viren/Lib/site-packages/werkzeug/debug/shared/ICON_LICENSE.html",
    "href": "viren/Lib/site-packages/werkzeug/debug/shared/ICON_LICENSE.html",
    "title": "IsabelleStokburger Blog",
    "section": "",
    "text": "Silk icon set 1.3 by Mark James mjames@gmail.com\nhttp://www.famfamfam.com/lab/icons/silk/\nLicense: CC-BY-2.5 or CC-BY-3.0"
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#datensatz-mit-tweets-ohne-stopwords-erstellen",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#datensatz-mit-tweets-ohne-stopwords-erstellen",
    "title": "Hate Speech Klassifikation",
    "section": "5.1 Datensatz mit Tweets ohne stopwords erstellen",
    "text": "5.1 Datensatz mit Tweets ohne stopwords erstellen\n\n# Benötigte Pakete laden\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(stopwords)\n\n# Stopwörter für Deutsch holen\nstopwords_en &lt;- stopwords::stopwords(language = \"en\")\n\n\nbigrame2 &lt;- d_train %&gt;% dplyr::select(class, tweet) %&gt;% unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2)\n\nbigram_sep &lt;- bigrame2 %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\nbi_filt &lt;- bigram_sep %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\nbigram_unite&lt;-bi_filt%&gt;%\n  unite(bigram, word1, word2, sep = \" \")\n  \n# Häufigkeit der Bigramme zählen\nbigram_freq &lt;- bigram_unite %&gt;%\n  count(bigram, sort = TRUE)\n\n# Die 10 häufigsten Bigramme auswählen\ntop_bigrams &lt;- head(bigram_freq, 10)\n\n# Überprüfen der Ergebnisse\nprint(top_bigrams)"
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#bigrame-visualisieren",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#bigrame-visualisieren",
    "title": "Hate Speech Klassifikation",
    "section": "5.4 Bigrame visualisieren",
    "text": "5.4 Bigrame visualisieren\n\ncomms_bigram &lt;- \n  d_train2 %&gt;%\n  unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\ncomms_bigram &lt;- comms_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ncomms_bigram %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram)) +\n  geom_col(fill = \"#8175aa\") +\n  labs(title = \"Bigramme nach Häufigkeit\",\n       x = \"Häufigkeit\",\n       y = \"Bigram\") +\n  theme_minimal()\n\n\n\n\nWhite Trash kommt mit Abstand am häufigsten vor. Das ist darauf zurückzuführen, dass trash sowohl in Verbindung mit “trash bins”, als auch als Schimpfwort und Redewendung verwendet wird. Derek Jeter ist ein bekannter Baseballspieler, der lange Zeit bei den Yankees war. Deshalb kommen die beiden Ausdrücke ungefähr gleich oft vor. Bei Charlie Christ handelt es sich um den ehemaliger Gouverneur von Florida und bei Charlie Sheen um einen sehr bekannten amerikanischen Schauspieler. Charlie Brown ist ein Charakter von den Peanuts. Die Bigrame lassen vermuten, dass die Themenstreuung der Tweets sehr breit ist."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#sprache-der-tweets",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#sprache-der-tweets",
    "title": "Hate Speech Klassifikation",
    "section": "3.7 Sprache der Tweets",
    "text": "3.7 Sprache der Tweets\n\nlibrary(reticulate)\npy_install(\"pandas\")\n\nUsing virtual environment \"C:/Users/isast/Documents/Laptop/HS Ansbach/5. Semester/Data Science II/Prüfung/IsaBlogneu/viren\" ...\n\npy_install(\"langdetect\")\n\nUsing virtual environment \"C:/Users/isast/Documents/Laptop/HS Ansbach/5. Semester/Data Science II/Prüfung/IsaBlogneu/viren\" ...\n\npy_install(\"matplotlib\")\n\nUsing virtual environment \"C:/Users/isast/Documents/Laptop/HS Ansbach/5. Semester/Data Science II/Prüfung/IsaBlogneu/viren\" ...\n\n\n\nimport pandas as pd\n\n# Achte darauf, doppelte Backslashes zu verwenden oder den String als Raw-String zu kennzeichnen\nfile_path = 'C:\\\\Users\\\\isast\\\\Documents\\\\Laptop\\\\HS Ansbach\\\\5. Semester\\\\Data Science II\\\\Prüfung\\\\d_hate.csv'\n\n# Verwende den korrekten Pfad, um deine CSV-Datei zu laden\ndf = pd.read_csv(file_path)\n\n# Jetzt kannst du mit deinem DataFrame in Python arbeiten\nprint(df.head())\n\n   id                                              tweet  class\n0   0  !!! RT @mayasolovely: As a woman you shouldn't...  other\n1  40    \" momma said no pussy cats inside my doghouse \"  other\n2  63  \"@Addicted2Guys: -SimplyAddictedToGuys http://...  other\n3  66  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...  other\n4  67  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...  other\n\n\n\nfrom langdetect import detect\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Funktion zur Erkennung der Sprache eines Tweets\ndef detect_language_from_tweet(tweet):\n    try:\n        # Erkenne die Sprache des Tweets\n        return detect(tweet)\n    except Exception as e:\n        # Im Fehlerfall gib 'unbekannt' zurück\n        return 'unbekannt'\n\n# Spracherkennung auf jeden Tweet anwenden\ndf['detected_language'] = df['tweet'].apply(detect_language_from_tweet)\n\n# Überprüfung der ersten paar Zeilen, um zu sehen, ob die Spracherkennung funktioniert hat\nprint(df[['tweet', 'detected_language']].head())\n\n                                               tweet detected_language\n0  !!! RT @mayasolovely: As a woman you shouldn't...                en\n1    \" momma said no pussy cats inside my doghouse \"                en\n2  \"@Addicted2Guys: -SimplyAddictedToGuys http://...                en\n3  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...                en\n4  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...                en\n\n\nGut zu wissen. Die Tweets sind fast ausschließlich in englisch. Wenige Ausnahmen, wie “af”, dass in der Spracherkennung normalerweise für Afrikaans, eine Sprache, die hauptsächlich in Südafrika und Namibia gesprochen wird, können bei der verschwindend geringen Menge vernachlässigt werden und es kann auf ein englisches Schimpfwort- & Sentimentlexikon zurückgegriffen werden."
  },
  {
    "objectID": "dsvenv/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "href": "dsvenv/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "title": "UAT for NbAgg backend.",
    "section": "",
    "text": "from imp import reload\nThe first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. Note: The underlying JavaScript will not be updated by this process, so a refresh of the browser after clearing the output and saving is necessary to clear everything fully.\nimport matplotlib\nreload(matplotlib)\n\nmatplotlib.use('nbagg')\n\nimport matplotlib.backends.backend_nbagg\nreload(matplotlib.backends.backend_nbagg)"
  },
  {
    "objectID": "dsvenv/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "href": "dsvenv/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "title": "UAT for NbAgg backend.",
    "section": "1 UAT 13 - Animation",
    "text": "1 UAT 13 - Animation\nThe following should generate an animated line:\n\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\n\nx = np.arange(0, 2*np.pi, 0.01)        # x-array\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x+i/10.0))  # update the data\n    return line,\n\n#Init only required for blitting to give a clean slate.\ndef init():\n    line.set_ydata(np.ma.array(x, mask=True))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n                              interval=100., blit=True)\nplt.show()\n\n\n1.1 UAT 14 - Keyboard shortcuts in IPython after close of figure\nAfter closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function.\n\n\n1.2 UAT 15 - Figure face colours\nThe nbagg honours all colours apart from that of the figure.patch. The two plots below should produce a figure with a red background. There should be no yellow figure.\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.facecolor': 'red',\n                            'savefig.facecolor': 'yellow'})\nplt.figure()\nplt.plot([3, 2, 1])\n\nplt.show()\n\n\n\n1.3 UAT 16 - Events\nPressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. The figure should have focus by default when it is created and re-gain it by clicking on the canvas. Clicking anywhere outside of the figure should release focus, but moving the mouse out of the figure should not release focus.\n\nimport itertools\nfig, ax = plt.subplots()\nx = np.linspace(0,10,10000)\ny = np.sin(x)\nln, = ax.plot(x,y)\nevt = []\ncolors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c']))\ndef on_event(event):\n    if event.name.startswith('key'):\n        fig.suptitle('%s: %s' % (event.name, event.key))\n    elif event.name == 'scroll_event':\n        fig.suptitle('%s: %s' % (event.name, event.step))\n    else:\n        fig.suptitle('%s: %s' % (event.name, event.button))\n    evt.append(event)\n    ln.set_color(next(colors))\n    fig.canvas.draw()\n    fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect('button_press_event', on_event)\nfig.canvas.mpl_connect('button_release_event', on_event)\nfig.canvas.mpl_connect('scroll_event', on_event)\nfig.canvas.mpl_connect('key_press_event', on_event)\nfig.canvas.mpl_connect('key_release_event', on_event)\n\nplt.show()\n\n\n\n1.4 UAT 17 - Timers\nSingle-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.) The next set of tests ensures that both “regular” and “single-shot” timers work properly.\nThe following should show a simple clock that updates twice a second:\n\nimport time\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\n\ndef update(text):\n    text.set(text=time.ctime())\n    text.axes.figure.canvas.draw()\n    \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\ntimer.start()\nplt.show()\n\nHowever, the following should only update once and then stop:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center') \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\n\nplt.show()\n\nAnd the next two examples should never show any visible text at all:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\n\n1.5 UAT 18 - stopping figure when removed from DOM\nWhen the div that contains from the figure is removed from the DOM the figure should shut down it’s comm, and if the python-side figure has no more active comms, it should destroy the figure. Repeatedly running the cell below should always have the same figure number\n\nfig, ax = plt.subplots()\nax.plot(range(5))\nplt.show()\n\nRunning the cell below will re-show the figure. After this, re-running the cell above should result in a new figure number.\n\nfig.canvas.manager.reshow()\n\n\n\n1.6 UAT 19 - Blitting\nClicking on the figure should plot a green horizontal line moving up the axes.\n\nimport itertools\n\ncnt = itertools.count()\nbg = None\n\ndef onclick_handle(event):\n    \"\"\"Should draw elevating green line on each mouse click\"\"\"\n    global bg\n    if bg is None:\n        bg = ax.figure.canvas.copy_from_bbox(ax.bbox) \n    ax.figure.canvas.restore_region(bg)\n\n    cur_y = (next(cnt) % 10) * 0.1\n    ln.set_ydata([cur_y, cur_y])\n    ax.draw_artist(ln)\n    ax.figure.canvas.blit(ax.bbox)\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], 'r')\nln, = ax.plot([0, 1], [0, 0], 'g', animated=True)\nplt.show()\nax.figure.canvas.draw()\n\nax.figure.canvas.mpl_connect('button_press_event', onclick_handle)"
  },
  {
    "objectID": "viren/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "href": "viren/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "title": "UAT for NbAgg backend.",
    "section": "",
    "text": "from imp import reload\nThe first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. Note: The underlying JavaScript will not be updated by this process, so a refresh of the browser after clearing the output and saving is necessary to clear everything fully.\nimport matplotlib\nreload(matplotlib)\n\nmatplotlib.use('nbagg')\n\nimport matplotlib.backends.backend_nbagg\nreload(matplotlib.backends.backend_nbagg)"
  },
  {
    "objectID": "viren/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "href": "viren/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "title": "UAT for NbAgg backend.",
    "section": "1 UAT 13 - Animation",
    "text": "1 UAT 13 - Animation\nThe following should generate an animated line:\n\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\n\nx = np.arange(0, 2*np.pi, 0.01)        # x-array\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x+i/10.0))  # update the data\n    return line,\n\n#Init only required for blitting to give a clean slate.\ndef init():\n    line.set_ydata(np.ma.array(x, mask=True))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n                              interval=100., blit=True)\nplt.show()\n\n\n1.1 UAT 14 - Keyboard shortcuts in IPython after close of figure\nAfter closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function.\n\n\n1.2 UAT 15 - Figure face colours\nThe nbagg honours all colours apart from that of the figure.patch. The two plots below should produce a figure with a red background. There should be no yellow figure.\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.facecolor': 'red',\n                            'savefig.facecolor': 'yellow'})\nplt.figure()\nplt.plot([3, 2, 1])\n\nplt.show()\n\n\n\n1.3 UAT 16 - Events\nPressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. The figure should have focus by default when it is created and re-gain it by clicking on the canvas. Clicking anywhere outside of the figure should release focus, but moving the mouse out of the figure should not release focus.\n\nimport itertools\nfig, ax = plt.subplots()\nx = np.linspace(0,10,10000)\ny = np.sin(x)\nln, = ax.plot(x,y)\nevt = []\ncolors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c']))\ndef on_event(event):\n    if event.name.startswith('key'):\n        fig.suptitle('%s: %s' % (event.name, event.key))\n    elif event.name == 'scroll_event':\n        fig.suptitle('%s: %s' % (event.name, event.step))\n    else:\n        fig.suptitle('%s: %s' % (event.name, event.button))\n    evt.append(event)\n    ln.set_color(next(colors))\n    fig.canvas.draw()\n    fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect('button_press_event', on_event)\nfig.canvas.mpl_connect('button_release_event', on_event)\nfig.canvas.mpl_connect('scroll_event', on_event)\nfig.canvas.mpl_connect('key_press_event', on_event)\nfig.canvas.mpl_connect('key_release_event', on_event)\n\nplt.show()\n\n\n\n1.4 UAT 17 - Timers\nSingle-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.) The next set of tests ensures that both “regular” and “single-shot” timers work properly.\nThe following should show a simple clock that updates twice a second:\n\nimport time\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\n\ndef update(text):\n    text.set(text=time.ctime())\n    text.axes.figure.canvas.draw()\n    \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\ntimer.start()\nplt.show()\n\nHowever, the following should only update once and then stop:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center') \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\n\nplt.show()\n\nAnd the next two examples should never show any visible text at all:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\n\n1.5 UAT 18 - stopping figure when removed from DOM\nWhen the div that contains from the figure is removed from the DOM the figure should shut down it’s comm, and if the python-side figure has no more active comms, it should destroy the figure. Repeatedly running the cell below should always have the same figure number\n\nfig, ax = plt.subplots()\nax.plot(range(5))\nplt.show()\n\nRunning the cell below will re-show the figure. After this, re-running the cell above should result in a new figure number.\n\nfig.canvas.manager.reshow()\n\n\n\n1.6 UAT 19 - Blitting\nClicking on the figure should plot a green horizontal line moving up the axes.\n\nimport itertools\n\ncnt = itertools.count()\nbg = None\n\ndef onclick_handle(event):\n    \"\"\"Should draw elevating green line on each mouse click\"\"\"\n    global bg\n    if bg is None:\n        bg = ax.figure.canvas.copy_from_bbox(ax.bbox) \n    ax.figure.canvas.restore_region(bg)\n\n    cur_y = (next(cnt) % 10) * 0.1\n    ln.set_ydata([cur_y, cur_y])\n    ax.draw_artist(ln)\n    ax.figure.canvas.blit(ax.bbox)\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], 'r')\nln, = ax.plot([0, 1], [0, 0], 'g', animated=True)\nplt.show()\nax.figure.canvas.draw()\n\nax.figure.canvas.mpl_connect('button_press_event', onclick_handle)"
  },
  {
    "objectID": "test sachen.html",
    "href": "test sachen.html",
    "title": "test",
    "section": "",
    "text": "import pandas as pd\n\n# Achte darauf, doppelte Backslashes zu verwenden oder den String als Raw-String zu kennzeichnen\nfile_path = 'C:\\\\Users\\\\isast\\\\Documents\\\\Laptop\\\\HS Ansbach\\\\5. Semester\\\\Data Science II\\\\Prüfung\\\\d_hate.csv'\n\n# Verwende den korrekten Pfad, um deine CSV-Datei zu laden\ndf = pd.read_csv(file_path)\n\n# Jetzt kannst du mit deinem DataFrame in Python arbeiten\nprint(df.head())\n\n   id                                              tweet  class\n0   0  !!! RT @mayasolovely: As a woman you shouldn't...  other\n1  40    \" momma said no pussy cats inside my doghouse \"  other\n2  63  \"@Addicted2Guys: -SimplyAddictedToGuys http://...  other\n3  66  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...  other\n4  67  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...  other\n\nfrom langdetect import detect\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Funktion zur Erkennung der Sprache eines Tweets\ndef detect_language_from_tweet(tweet):\n    try:\n        # Erkenne die Sprache des Tweets\n        return detect(tweet)\n    except Exception as e:\n        # Im Fehlerfall gib 'unbekannt' zurück\n        return 'unbekannt'\n\n# Spracherkennung auf jeden Tweet anwenden\ndf['detected_language'] = df['tweet'].apply(detect_language_from_tweet)\n\n# Überprüfung der ersten paar Zeilen, um zu sehen, ob die Spracherkennung funktioniert hat\nprint(df[['tweet', 'detected_language']].head())\n\n                                               tweet detected_language\n0  !!! RT @mayasolovely: As a woman you shouldn't...                en\n1    \" momma said no pussy cats inside my doghouse \"                en\n2  \"@Addicted2Guys: -SimplyAddictedToGuys http://...                en\n3  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...                en\n4  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...                en\n\n\n\n# Dieser Code hängt davon ab, dass der vorherige Python-Code erfolgreich ausgeführt wurde\n# und 'df' ist der Name des Pandas DataFrame in Python\n\n# Aktivieren Sie die Python-Engine\nlibrary(reticulate)\n\nWarning: Paket 'reticulate' wurde unter R Version 4.3.2 erstellt\n\n# Laden Sie den DataFrame in R\ndf_r &lt;- py$df\n\n# Überprüfen Sie die ersten paar Zeilen des DataFrame\nhead(df_r)\n\n\n\n  \n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttache Paket: 'dplyr'\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Zählen Sie die Anzahl der Tweets pro Sprache\nlanguage_counts_df &lt;- df_r %&gt;%\n  count(detected_language) %&gt;%\n  arrange(desc(n))\n\n# Erstellen Sie ein Balkendiagramm der Sprachverteilung\nggplot(language_counts_df, aes(x = detected_language, y = n, fill = detected_language)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = 'Verteilung der erkannten Sprachen in Tweets',\n       x = 'Sprache',\n       y = 'Anzahl der Tweets') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # X-Achsen-Beschriftungen rotieren"
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage-mit-logistic-regression",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage-mit-logistic-regression",
    "title": "Hate Speech Klassifikation",
    "section": "7.1 1. Vorhersage mit logistic regression",
    "text": "7.1 1. Vorhersage mit logistic regression\nAuf das Tunen wird bei der ersten Vorhersage verzichtet, um die Komplexität des Modells gering zu halten und somit Rechenzeit zu sparen."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage-hugging-face-modell",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#vorhersage-hugging-face-modell",
    "title": "Hate Speech Klassifikation",
    "section": "8.1 3. Vorhersage: Hugging face modell",
    "text": "8.1 3. Vorhersage: Hugging face modell\nFür die dritte Vorhersage nutze ich ein neuronales Netz, dass bereits trainiert wurde und von Hugging Face bezogen wird.\n\nlibrary(reticulate)\n\nWarning: Paket 'reticulate' wurde unter R Version 4.3.2 erstellt\n\n\n\nuse_virtualenv(\"~/Laptop/HS Ansbach/5. Semester/Data Science II/Prüfung/IsaBlogneu/viren\")\n\n\nfrom transformers import pipeline\n\nWARNING:tensorflow:From C:\\Users\\isast\\DOCUME~1\\Laptop\\HSANSB~1\\58A3C~1.SEM\\DATASC~1\\PRFUNG~1\\ISABLO~1\\viren\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\nimport tensorflow as tf\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\n\ntweets &lt;- d_test$tweet\n\n\ntweets = r.tweets\nresults = classifier(tweets)\n\n\n# Extraktion der 'name'-Werte aus der Liste 'py$results'\nextracted_names &lt;- lapply(py$results, function(item) item$label)\n\n# Zusammenfügen der vorhergesagten Namen (vereinheitlicht) mit 'd_test' in eine neue Spalte 'prediction'\ntwitter_analysis &lt;- cbind(d_test, prediction = unlist(extracted_names))\n\n# Umwandlung der 'class'-Spalte in einen Faktor und Anpassung der 'prediction'-Spalte\ntwitter_analysis &lt;- twitter_analysis %&gt;%\n  mutate(class = factor(class),\n         prediction = ifelse(prediction == \"hate\", \"hate speech\", \"other\"),\n         prediction = factor(prediction))\n\n\nmy_metrics2 &lt;- metric_set(accuracy, f_meas)\nmy_metrics2(twitter_analysis,\n           truth = class,\n           estimate = prediction)\n\n\n\n  \n\n\n\nDas Modell hat eine Genauigkeit von 91%. Das nenne ich traumhaft. Kein Vergleich zu den vorherigen Modellen."
  },
  {
    "objectID": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#nachtrag-eda-sprache-der-tweets",
    "href": "posts/Prüfung/Pruefung_Isabelle_Stokburger.html#nachtrag-eda-sprache-der-tweets",
    "title": "Hate Speech Klassifikation",
    "section": "8.2 Nachtrag EDA: Sprache der Tweets",
    "text": "8.2 Nachtrag EDA: Sprache der Tweets\n\npy_install(\"pandas\")\n\nUsing virtual environment \"C:/Users/isast/Documents/Laptop/HS Ansbach/5. Semester/Data Science II/Prüfung/IsaBlogneu/viren\" ...\n\npy_install(\"langdetect\")\n\nUsing virtual environment \"C:/Users/isast/Documents/Laptop/HS Ansbach/5. Semester/Data Science II/Prüfung/IsaBlogneu/viren\" ...\n\npy_install(\"matplotlib\")\n\nUsing virtual environment \"C:/Users/isast/Documents/Laptop/HS Ansbach/5. Semester/Data Science II/Prüfung/IsaBlogneu/viren\" ...\n\n\n\nimport pandas as pd\n\nfile_path = 'C:\\\\Users\\\\isast\\\\Documents\\\\Laptop\\\\HS Ansbach\\\\5. Semester\\\\Data Science II\\\\Prüfung\\\\d_hate.csv'\n\ndf = pd.read_csv(file_path)\n\nprint(df.head())\n\n   id                                              tweet  class\n0   0  !!! RT @mayasolovely: As a woman you shouldn't...  other\n1  40    \" momma said no pussy cats inside my doghouse \"  other\n2  63  \"@Addicted2Guys: -SimplyAddictedToGuys http://...  other\n3  66  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...  other\n4  67  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...  other\n\n\n\nfrom langdetect import detect\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Funktion zur Erkennung der Sprache eines Tweets\ndef detect_language_from_tweet(tweet):\n    try:\n        # Erkenne die Sprache des Tweets\n        return detect(tweet)\n    except Exception as e:\n        # Im Fehlerfall gib 'unbekannt' zurück\n        return 'unbekannt'\n\n# Spracherkennung auf jeden Tweet anwenden\ndf['detected_language'] = df['tweet'].apply(detect_language_from_tweet)\n\n# Überprüfung der ersten paar Zeilen, um zu sehen, ob die Spracherkennung funktioniert hat\nprint(df[['tweet', 'detected_language']].head())\n\n                                               tweet detected_language\n0  !!! RT @mayasolovely: As a woman you shouldn't...                en\n1    \" momma said no pussy cats inside my doghouse \"                en\n2  \"@Addicted2Guys: -SimplyAddictedToGuys http://...                en\n3  \"@AllAboutManFeet: http://t.co/3gzUpfuMev\" woo...                en\n4  \"@Allyhaaaaa: Lemmie eat a Oreo &amp; do these...                en\n\n\n\ndf_r &lt;- py$df\n\nhead(df_r)\n\n\n\n  \n\n\n\n\n# Anzahl der Tweets pro Sprache\nlanguage_counts_df &lt;- df_r %&gt;%\n  count(detected_language) %&gt;%\n  arrange(desc(n))\n\n# Balkendiagramm der Sprachverteilung\nggplot(language_counts_df, aes(x = detected_language, y = n, fill = detected_language)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal() +\n  labs(title = 'Verteilung der erkannten Sprachen in Tweets',\n       x = 'Sprache',\n       y = 'Anzahl der Tweets') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\nGut zu wissen. Die Tweets sind fast ausschließlich in englisch. Wenige Ausnahmen, wie “af”, dass in der Spracherkennung normalerweise für Afrikaans, eine Sprache, die hauptsächlich in Südafrika und Namibia gesprochen wird, können bei der verschwindend geringen Menge vernachlässigt werden und es kann auf ein englisches Schimpfwort- & Sentimentlexikon zurückgegriffen werden."
  }
]